sma_replace_na <- function(x, n = 3) {
# Ensure x is a numeric vector
x <- as.numeric(x)
# Compute the moving average with a window size of 'n', ignoring NA values
sma <- rollapply(x, width = n, FUN = function(y) mean(y, na.rm = TRUE), fill = NA, align = "right")
# Replace the NA values in the original series with the computed moving average values
x[is.na(x)] <- sma[is.na(x)]
return(x)
}
# Apply the function to each column except the date column
for (col in names(transformed_data_cleaned)) {
if (col != "date") {
transformed_data_cleaned[[col]] <- sma_replace_na(transformed_data_cleaned[[col]])
}
}
# Display the cleaned dataframe
print(transformed_data_cleaned)
# NOTE: The SMA method calculates the average of the previous 'n' values in a series and uses this average to replace NA values. This helps in smoothing the data by filling in missing values based on the surrounding data points, thereby maintaining the overall trend and continuity of the data.
################################################################################
# Re-check missing values for data transformed cleaned
# Count and locate NAs in transformed data cleaned
na_details_transformed_data_cleaned <- count_and_locate_nas(transformed_data_cleaned)
print("Details of NAs in transformed data cleaned:")
print(na_details_transformed_data_cleaned)
################################################################################
# Take COVID-19 years out
################################################################################
# Remove the years
transformed_data_cleaned_no_COVID <- transformed_data_cleaned[as.Date(transformed_data_cleaned$date) <= as.Date("2019-12-01"), ]
# Display the first few rows of the cleaned data
head(transformed_data_cleaned_no_COVID)
################################################################################
# Lagging and Splitting the Data
################################################################################
# Define X (predictors, past values) and Y (target, future values) with the `date` column retained
X_before_splitting <- transformed_data_cleaned_no_COVID[-nrow(transformed_data_cleaned_no_COVID), ]  # Remove last row for X (past values)
Y_before_splitting <- transformed_data_cleaned_no_COVID[-1, c("date", "CPIULFSL")]  # Remove first row for Y (future values), keeping date
################################################################################
# Holdout Method for Splitting Data
################################################################################
# Define the split date (we use 70%)
split_date <- as.Date("2000-01-01")
# Split the data based on the date
train_indices <- X_before_splitting$date < split_date
test_indices <- X_before_splitting$date >= split_date
# Split X and Y into train and test sets (date column still included for both X and Y)
X_train_with_date <- X_before_splitting[train_indices, ]
Y_train_with_date <- Y_before_splitting[train_indices, ]
X_test_with_date <- X_before_splitting[test_indices, ]
Y_test_with_date <- Y_before_splitting[test_indices, ]
# Remove the date column after splitting for X and Y
X_train <- X_train_with_date[, -1]  # Remove the date column
Y_train <- Y_train_with_date[, "CPIULFSL"]
X_test <- X_test_with_date[, -1]  # Remove the date column
Y_test <- Y_test_with_date[, "CPIULFSL"]
################################################################################
# Verify the split
################################################################################
cat("Training Data Date Range:\n")
print(range(X_train_with_date$date))
cat("\nTest Data Date Range:\n")
print(range(X_test_with_date$date))
# Check the dimensions of the splits
cat("\nTraining Data Dimensions:\n")
print(dim(X_train))
cat(length(Y_train), "\n")  # Check length of Y_train
cat("\nTest Data Dimensions:\n")
print(dim(X_test))
cat(length(Y_test), "\n")  # Check length of Y_test
suppressWarnings(suppressMessages({
library(stats)
library(vars)
library(ggplot2)
}))
################################################################################
################################################################################
# Function: Perform VAR with Dynamic PCA Configurations
################################################################################
suppressWarnings(suppressMessages({
library(stats)
library(vars)
library(ggplot2)
}))
perform_var_with_pca <- function(num_components, title, output_folder) {
# Create output folder if it doesn't exist
if (!dir.exists(output_folder)) {
dir.create(output_folder)
}
# Initialize variables for recursive rolling predictions
Y_train_vec <- as.numeric(Y_train)
X_train_matrix <- as.matrix(X_train)
Y_test_vec <- as.numeric(Y_test)
X_test_matrix <- as.matrix(X_test)
n_test <- length(Y_test_vec)
predictions <- numeric(n_test)
for (i in 1:n_test) {
# Step 1: Standardize the current training data
mean_Y_train <- mean(Y_train_vec)
sd_Y_train <- sd(Y_train_vec)
Y_train_standardized <- (Y_train_vec - mean_Y_train) / sd_Y_train
mean_X_train <- colMeans(X_train_matrix)
sd_X_train <- apply(X_train_matrix, 2, sd)
X_train_standardized <- scale(X_train_matrix, center = mean_X_train, scale = sd_X_train)
# Step 2: Apply PCA dynamically to the standardized training data
pca_model <- prcomp(X_train_standardized, center = FALSE, scale. = FALSE)
X_train_pca <- pca_model$x[, 1:num_components]  # Use specified number of components
# Step 3: Fit the VAR model using the selected principal components
train_data_standardized <- data.frame(Y = Y_train_standardized, X_train_pca)
var_model <- VAR(train_data_standardized, p = 1)
# Step 4: Predict using the VAR model
new_X <- matrix(X_test_matrix[i, ], nrow = 1)  # Ensure new_X is a 1-row matrix
new_X <- scale(new_X, center = mean_X_train, scale = sd_X_train)
# Add column names to match X_train
colnames(new_X) <- colnames(X_train_matrix)
new_X_pca <- predict(pca_model, newdata = as.data.frame(new_X))[, 1:num_components]
new_test_data <- data.frame(t(new_X_pca))
colnames(new_test_data) <- colnames(train_data_standardized)[-1]  # Exclude Y column
var_forecast <- predict(var_model, n.ahead = 1)
prediction_standardized <- var_forecast$fcst$Y[1, "fcst"]
# Step 5: De-standardize the prediction
prediction <- prediction_standardized * sd_Y_train + mean_Y_train
predictions[i] <- prediction
# Update training data
actual_value <- Y_test_vec[i]
Y_train_vec <- c(Y_train_vec, actual_value)
X_train_matrix <- rbind(X_train_matrix, X_test_matrix[i, ])
}
# Metrics
test_errors <- Y_test_vec - predictions
test_mae <- mean(abs(test_errors))
test_mse <- mean(test_errors^2)
test_rmse <- sqrt(test_mse)
# Print metrics
cat("Number of Principal Components:", num_components, "\n")
cat("MAE:", test_mae, "MSE:", test_mse, "RMSE:", test_rmse, "\n\n")
# Create diagnostic plots
df_test <- data.frame(Date = Y_test_with_date$date, Actual = Y_test_vec, Predicted = predictions)
# Time series plot
time_series_plot <- ggplot(df_test, aes(x = Date)) +
geom_line(aes(y = Actual, color = "Actual")) +
geom_line(aes(y = Predicted, color = "Predicted")) +
labs(title = title, x = "Date", y = "CPIULFSL", color = "Legend") +
theme_minimal()
ggsave(file.path(output_folder, paste0("var_", num_components, "_pcs_timeseries.pdf")),
plot = time_series_plot, width = 7, height = 7, dpi = 300)
}
# Define PCA configurations and output folder
pc_values <- c(3, 5, 10, 15, 20, 25, 30, 35, 40)
output_folder <- "var_pca_results"
# Loop through each PCA configuration
for (num_components in pc_values) {
title <- paste("VAR with", num_components, "Principal Components")
perform_var_with_pca(num_components, title, output_folder)
}
suppressWarnings(suppressMessages({
library(stats)
library(vars)
library(ggplot2)
}))
perform_var_with_pca <- function(num_components, title, output_folder) {
# Create output folder if it doesn't exist
if (!dir.exists(output_folder)) {
dir.create(output_folder)
}
# Initialize variables for recursive rolling predictions
Y_train_vec <- as.numeric(Y_train)
X_train_matrix <- as.matrix(X_train)
Y_test_vec <- as.numeric(Y_test)
X_test_matrix <- as.matrix(X_test)
n_test <- length(Y_test_vec)
predictions <- numeric(n_test)
for (i in 1:n_test) {
# Step 1: Standardize the current training data
mean_Y_train <- mean(Y_train_vec)
sd_Y_train <- sd(Y_train_vec)
Y_train_standardized <- (Y_train_vec - mean_Y_train) / sd_Y_train
mean_X_train <- colMeans(X_train_matrix)
sd_X_train <- apply(X_train_matrix, 2, sd)
X_train_standardized <- scale(X_train_matrix, center = mean_X_train, scale = sd_X_train)
# Step 2: Apply PCA dynamically to the standardized training data
pca_model <- prcomp(X_train_standardized, center = FALSE, scale. = FALSE)
X_train_pca <- pca_model$x[, 1:num_components]  # Use specified number of components
# Step 3: Fit the VAR model using the selected principal components
train_data_standardized <- data.frame(Y = Y_train_standardized, X_train_pca)
var_model <- VAR(train_data_standardized, p = 1)
# Step 4: Predict using the VAR model
new_X <- matrix(X_test_matrix[i, ], nrow = 1)  # Ensure new_X is a 1-row matrix
new_X <- scale(new_X, center = mean_X_train, scale = sd_X_train)
colnames(new_X) <- colnames(X_train_matrix)  # Match column names
new_X_pca <- predict(pca_model, newdata = as.data.frame(new_X))[, 1:num_components]
new_test_data <- data.frame(t(new_X_pca))
colnames(new_test_data) <- colnames(train_data_standardized)[-1]  # Exclude Y column
var_forecast <- predict(var_model, n.ahead = 1)
prediction_standardized <- var_forecast$fcst$Y[1, "fcst"]
# Step 5: De-standardize the prediction
prediction <- prediction_standardized * sd_Y_train + mean_Y_train
predictions[i] <- prediction
# Update training data
actual_value <- Y_test_vec[i]
Y_train_vec <- c(Y_train_vec, actual_value)
X_train_matrix <- rbind(X_train_matrix, X_test_matrix[i, ])
}
# Metrics
test_errors <- Y_test_vec - predictions
test_mae <- mean(abs(test_errors))
test_mse <- mean(test_errors^2)
test_rmse <- sqrt(test_mse)
cat("Number of Principal Components:", num_components, "\n")
cat("MAE:", test_mae, "MSE:", test_mse, "RMSE:", test_rmse, "\n\n")
# Create diagnostic data frame
df_test <- data.frame(Date = Y_test_with_date$date, Actual = Y_test_vec, Predicted = predictions)
# Time Series Plot
time_series_plot <- ggplot(df_test, aes(x = Date)) +
geom_line(aes(y = Actual, color = "Actual")) +
geom_line(aes(y = Predicted, color = "Predicted")) +
labs(title = title, x = "Date", y = "CPIULFSL", color = "Legend") +
theme_minimal() +
scale_color_manual(values = c("Actual" = "blue", "Predicted" = "red"))
ggsave(file.path(output_folder, paste0("var_", num_components, "_pcs_timeseries.pdf")),
plot = time_series_plot, width = 10, height = 8, dpi = 300, units = "in")
# Residual Plot
residual_plot <- ggplot(df_test, aes(x = Actual, y = Actual - Predicted)) +
geom_point() +
geom_hline(yintercept = 0, color = "red") +
ggtitle(paste("VAR Residual Plot (Components =", num_components, ")")) +
labs(x = "Actual", y = "Residual (Actual - Predicted)") +
theme_minimal()
ggsave(file.path(output_folder, paste0("var_", num_components, "_pcs_residuals.pdf")),
plot = residual_plot, width = 10, height = 8, dpi = 300, units = "in")
# Scatter Plot
scatter_plot <- ggplot(df_test, aes(x = Actual, y = Predicted)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red") +
ggtitle(paste("VAR: Actual vs Predicted (Components =", num_components, ")")) +
labs(x = "Actual", y = "Predicted") +
theme_minimal()
ggsave(file.path(output_folder, paste0("var_", num_components, "_pcs_scatter.pdf")),
plot = scatter_plot, width = 10, height = 8, dpi = 300, units = "in")
}
pc_values <- c(3, 5, 10, 15, 20, 25, 30, 35, 40)
output_folder <- "var_pca_results"
for (num_components in pc_values) {
title <- paste("VAR with", num_components, "Principal Components")
perform_var_with_pca(num_components, title, output_folder)
}
# Load packages quietly
# Suppress warnings and messages
suppressWarnings(suppressMessages({
library(pls)
library(glmnet)
library(dplyr)
library(forecast)
library(tidyverse)
library(readr)
library(lubridate)
library(zoo)
library(ggplot2)
library(reshape2)
library(viridis)
library(fbi)
library(utils)
library(imputeTS)
library(randomForest)
library(vars)
}))
# Define the fredmd function
fredmd <- function(file = "", date_start = NULL, date_end = NULL, transform = TRUE) {
# Debug: print the file path
print(paste("File:", file))
# Error checking
if (!is.logical(transform))
stop("'transform' must be logical.")
if ((class(date_start) != "Date") && (!is.null(date_start)))
stop("'date_start' must be Date or NULL.")
if ((class(date_end) != "Date") && (!is.null(date_end)))
stop("'date_end' must be Date or NULL.")
if (class(date_start) == "Date") {
if (as.numeric(format(date_start, "%d")) != 1)
stop("'date_start' must be Date whose day is 1.")
if (date_start < as.Date("1959-01-01"))
stop("'date_start' must be later than 1959-01-01.")
}
if (class(date_end) == "Date") {
if (as.numeric(format(date_end, "%d")) != 1)
stop("'date_end' must be Date whose day is 1.")
}
print("Reading raw data...")
# Prepare raw data
rawdata <- readr::read_csv(file, col_names = FALSE, col_types = cols(X1 = col_date(format = "%m/%d/%Y")), skip = 2)
print(head(rawdata))
rawdata <- as.data.frame(rawdata)
row_to_remove = c()
for (row in (nrow(rawdata) - 20):nrow(rawdata)) {
if (!any(is.finite(unlist(rawdata[row, ])))) {
row_to_remove = c(row_to_remove, row)  # remove NA rows
}
}
if (length(row_to_remove) > 0) {
rawdata = rawdata[-row_to_remove, ]
}
print("Raw data after removing NA rows:")
print(head(rawdata))
print("Reading attribute data...")
attrdata <- utils::read.csv(file, header = FALSE, nrows = 2)
header <- c("date", unlist(attrdata[1, 2:ncol(attrdata)]))
colnames(rawdata) <- header
print("Header:")
print(header)
# Store transformation codes as tcode
tcode <- unlist(attrdata[2, 2:ncol(attrdata)])
print("Transformation codes:")
print(tcode)
# Subfunction transxf: data transformation based on tcodes
transxf <- function(x, tcode) {
# Number of observations (including missing values)
n <- length(x)
# Value close to zero
small <- 1e-06
# Allocate output variable
y <- rep(NA, n)
y1 <- rep(NA, n)
# TRANSFORMATION: Determine case 1-7 by transformation code
if (tcode == 1) {
# Case 1 Level (i.e. no transformation): x(t)
y <- x
} else if (tcode == 2) {
# Case 2 First difference: x(t)-x(t-1)
y[2:n] <- x[2:n] - x[1:(n - 1)]
} else if (tcode == 3) {
# case 3 Second difference: (x(t)-x(t-1))-(x(t-1)-x(t-2))
y[3:n] <- x[3:n] - 2 * x[2:(n - 1)] + x[1:(n - 2)]
} else if (tcode == 4) {
# case 4 Natural log: ln(x)
if (min(x, na.rm = TRUE) > small)
y <- log(x)
} else if (tcode == 5) {
# case 5 First difference of natural log: ln(x)-ln(x-1)
if (min(x, na.rm = TRUE) > small) {
x <- log(x)
y[2:n] <- x[2:n] - x[1:(n - 1)]
}
} else if (tcode == 6) {
# case 6 Second difference of natural log:
# (ln(x)-ln(x-1))-(ln(x-1)-ln(x-2))
if (min(x, na.rm = TRUE) > small) {
x <- log(x)
y[3:n] <- x[3:n] - 2 * x[2:(n - 1)] + x[1:(n - 2)]
}
} else if (tcode == 7) {
# case 7 First difference of percent change:
# (x(t)/x(t-1)-1)-(x(t-1)/x(t-2)-1)
y1[2:n] <- (x[2:n] - x[1:(n - 1)]) / x[1:(n - 1)]
y[3:n] <- y1[3:n] - y1[2:(n - 1)]
}
return(y)
}
# Transform data
if (transform) {
# Apply transformations
N <- ncol(rawdata)
data <- rawdata
data[, 2:N] <- NA
# Perform transformation using subfunction transxf (see below for details)
for (i in 2:N) {
temp <- transxf(rawdata[, i], tcode[i - 1])
data[, i] <- temp
}
} else {
data <- rawdata
}
print("Data after transformation:")
print(head(data))
# Null case of date_start and date_end
if (is.null(date_start))
date_start <- as.Date("1959-01-01")
if (is.null(date_end))
date_end <- data[, 1][nrow(data)]
# Subset data
index_start <- which.max(data[, 1] == date_start)
index_end <- which.max(data[, 1] == date_end)
outdata <- data[index_start:index_end, ]
class(outdata) <- c("data.frame", "fredmd")
return(outdata)
}
# Set wd and load dataset
file_path <- "./current.csv"
dataset <- read.csv(file_path)
# Load transformed data
transformed_data <- fredmd(file = file_path, transform = TRUE)
print("Transformed Data:")
print(head(transformed_data))
# Load original (non-transformed) data
original_data <- fredmd(file = file_path, transform = FALSE)
print("Original Data:")
print(head(original_data))
# Check the structure and summary of transformed_data
str(transformed_data)
summary(transformed_data)
# Check the structure and summary of original_data
str(original_data)
summary(original_data)
data(fredmd_description) # Click on fredmd_description <Promise> on the Environment
print(fredmd_description)
# Identify rows with NA values
na_rows_original_data <- which(rowSums(is.na(original_data)) > 0)
# Extract the dates corresponding to those rows
dates_with_na_original_data <- original_data$date[na_rows_original_data]
# Print the dates with NA values
print("Dates with NA values for original data:")
print(dates_with_na_original_data)
# Transformed data inspection (check for missing values)
# Identify rows with NA values
na_rows_transformed_data <- which(rowSums(is.na(transformed_data)) > 0)
# Extract the dates corresponding to those rows
dates_with_na_transformed_data <- transformed_data$date[na_rows_transformed_data]
# Print the dates with NA values
print("Dates with NA values for transformed data:")
print(dates_with_na_transformed_data)
# Function to count and locate NA values in a dataset
count_and_locate_nas <- function(data) {
# Get positions of NAs
na_positions <- which(is.na(data), arr.ind = TRUE)
# Extract column names and row names (dates) with NAs
variables_with_nas <- colnames(data)[na_positions[, 2]]
dates_with_nas <- data$date[na_positions[, 1]]
# Create a data frame with the results
na_details <- data.frame(Date = dates_with_nas, Variable = variables_with_nas)
return(na_details)
}
# Check for 'date' column and ensure it doesn't contain NAs
if (!("date" %in% names(original_data)) || any(is.na(original_data$date))) {
stop("The 'date' column is missing or contains NA values in the original data.")
}
if (!("date" %in% names(transformed_data)) || any(is.na(transformed_data$date))) {
stop("The 'date' column is missing or contains NA values in the transformed data.")
}
# Count and locate NAs in original data
na_details_original_data <- count_and_locate_nas(original_data)
# Count and locate NAs in transformed data
na_details_transformed_data <- count_and_locate_nas(transformed_data)
# Print the results
print("Details of NAs in original data:")
print(na_details_original_data)
print("Details of NAs in transformed data:")
print(na_details_transformed_data)
# Convert the fredmd data frame to a regular data frame if not already done
transformed_data_df <- as.data.frame(transformed_data)
# Remove rows where date is before 1960-01-01
transformed_data_cleaned <- transformed_data_df[as.Date(transformed_data_df$date) >= as.Date("1960-01-01"), ]
# Remove last row with NAs
transformed_data_cleaned <- transformed_data_cleaned[as.Date(transformed_data_cleaned$date) <= as.Date("2024-01-01"), ]
# Remove specific columns with too many NAs
columns_to_remove <- c("ACOGNO", "ANDENOx", "TWEXAFEGSMTHx", "UMCSENTx", "VIXCLSx")
transformed_data_cleaned <- transformed_data_cleaned[, !names(transformed_data_cleaned) %in% columns_to_remove]
# Display the first few rows of the cleaned data
head(transformed_data_cleaned)
################################################################################
# Missing values for data transformed cleaned
# Count and locate NAs in transformed data cleaned
na_details_transformed_data_cleaned <- count_and_locate_nas(transformed_data_cleaned)
print("Details of NAs in transformed data cleaned:")
print(na_details_transformed_data_cleaned)
################################################################################
# Cleaning dataset from the last nas (we use SMA for the remaining nas)
# Ensure transformed_data_cleaned is a dataframe
transformed_data_cleaned <- as.data.frame(transformed_data_cleaned)
# Define the function to replace NA values with simple moving average
sma_replace_na <- function(x, n = 3) {
# Ensure x is a numeric vector
x <- as.numeric(x)
# Compute the moving average with a window size of 'n', ignoring NA values
sma <- rollapply(x, width = n, FUN = function(y) mean(y, na.rm = TRUE), fill = NA, align = "right")
# Replace the NA values in the original series with the computed moving average values
x[is.na(x)] <- sma[is.na(x)]
return(x)
}
# Apply the function to each column except the date column
for (col in names(transformed_data_cleaned)) {
if (col != "date") {
transformed_data_cleaned[[col]] <- sma_replace_na(transformed_data_cleaned[[col]])
}
}
# Display the cleaned dataframe
print(transformed_data_cleaned)
################################################################################
# Re-check missing values for data transformed cleaned
# Count and locate NAs in transformed data cleaned
na_details_transformed_data_cleaned <- count_and_locate_nas(transformed_data_cleaned)
print("Details of NAs in transformed data cleaned:")
print(na_details_transformed_data_cleaned)
# Remove the years
transformed_data_cleaned_no_COVID <- transformed_data_cleaned[as.Date(transformed_data_cleaned$date) <= as.Date("2019-12-01"), ]
# Display the first few rows of the cleaned data
head(transformed_data_cleaned_no_COVID)
# Define X (predictors, past values) and Y (target, future values) with the `date` column retained
X_before_splitting <- transformed_data_cleaned_no_COVID[-nrow(transformed_data_cleaned_no_COVID), ]  # Remove last row for X (past values)
Y_before_splitting <- transformed_data_cleaned_no_COVID[-1, c("date", "CPIULFSL")]  # Remove first row for Y (future values), keeping date
# Define the split date (we use 70%)
split_date <- as.Date("2000-01-01")
# Split the data based on the date
train_indices <- X_before_splitting$date < split_date
test_indices <- X_before_splitting$date >= split_date
# Split X and Y into train and test sets (date column still included for both X and Y)
X_train_with_date <- X_before_splitting[train_indices, ]
Y_train_with_date <- Y_before_splitting[train_indices, ]
X_test_with_date <- X_before_splitting[test_indices, ]
Y_test_with_date <- Y_before_splitting[test_indices, ]
View(X_test_with_date)
View(X_train_with_date)
View(X_train_with_date)
View(Y_train_with_date)
View(Y_test_with_date)
